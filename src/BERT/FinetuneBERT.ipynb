{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定包含JSON文件的文件夹路径\n",
    "folder_path = '../data/esg_label_result'\n",
    "\n",
    "# 使用glob获取文件夹中所有的JSON文件\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# 逐个读取每个JSON文件\n",
    "for file in json_files:\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        all_data.extend(data)  # 将所有JSON文件的数据合并到一个列表中\n",
    "\n",
    "# # 打印合并后的数据\n",
    "# print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取JSON文件 \n",
    "# with open('../data/esg_label_result/AF Global Limited_report_filtered.json', 'r') as f:\n",
    "#     data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [item for item in all_data if \"error\" not in item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict ={\n",
    "        \"B-ENV_GHG_AET\": 0,\n",
    "        \"I-ENV_GHG_AET\": 0,\n",
    "        \"B-ENV_GHG_AE1\": 0,\n",
    "        \"I-ENV_GHG_AE1\": 0,\n",
    "        \"B-ENV_GHG_AE2\": 0,\n",
    "        \"I-ENV_GHG_AE2\": 0,\n",
    "        \"B-ENV_GHG_AE3\": 0,\n",
    "        \"I-ENV_GHG_AE3\": 0,\n",
    "        \"B-ENV_GHG_EIT\": 0,\n",
    "        \"I-ENV_GHG_EIT\": 0,\n",
    "        \"B-ENV_GHG_EI1\": 0,\n",
    "        \"I-ENV_GHG_EI1\": 0,\n",
    "        \"B-ENV_GHG_EI2\": 0,\n",
    "        \"I-ENV_GHG_EI2\": 0,\n",
    "        \"B-ENV_GHG_EI3\": 0,\n",
    "        \"I-ENV_GHG_EI3\": 0,\n",
    "        \"B-ENV_ENC_TEC\": 0,\n",
    "        \"I-ENV_ENC_TEC\": 0,\n",
    "        \"B-ENV_ENC_ECI\": 0,\n",
    "        \"I-ENV_ENC_ECI\": 0,\n",
    "        \"B-ENV_WAC_TWC\": 0,\n",
    "        \"I-ENV_WAC_TWC\": 0,\n",
    "        \"B-ENV_WAC_WCI\": 0,\n",
    "        \"I-ENV_WAC_WCI\": 0,\n",
    "        \"B-ENV_WAG_TWG\": 0,\n",
    "        \"I-ENV_WAG_TWG\": 0,\n",
    "        \"B-SOC_GED_CEG_M\": 0,\n",
    "        \"I-SOC_GED_CEG_M\": 0,\n",
    "        \"B-SOC_GED_CEG_F\": 0,\n",
    "        \"I-SOC_GED_CEG_F\": 0,\n",
    "        \"B-SOC_GED_NHG_M\": 0,\n",
    "        \"I-SOC_GED_NHG_M\": 0,\n",
    "        \"B-SOC_GED_NHG_F\": 0,\n",
    "        \"I-SOC_GED_NHG_F\": 0,\n",
    "        \"B-SOC_GED_ETG_M\": 0,\n",
    "        \"I-SOC_GED_ETG_M\": 0,\n",
    "        \"B-SOC_GED_ETG_F\": 0,\n",
    "        \"I-SOC_GED_ETG_F\": 0,\n",
    "        \"B-SOC_AGD_CEA_U30\": 0,\n",
    "        \"I-SOC_AGD_CEA_U30\": 0,\n",
    "        \"B-SOC_AGD_CEA_B35\": 0,\n",
    "        \"I-SOC_AGD_CEA_B35\": 0,\n",
    "        \"B-SOC_AGD_CEA_A50\": 0,\n",
    "        \"I-SOC_AGD_CEA_A50\": 0,\n",
    "        \"B-SOC_AGD_NHI_U30\": 0,\n",
    "        \"I-SOC_AGD_NHI_U30\": 0,\n",
    "        \"B-SOC_AGD_NHI_B35\": 0,\n",
    "        \"I-SOC_AGD_NHI_B35\": 0,\n",
    "        \"B-SOC_AGD_NHI_A50\": 0,\n",
    "        \"I-SOC_AGD_NHI_A50\": 0,\n",
    "        \"B-SOC_AGD_TOR_U30\": 0,\n",
    "        \"I-SOC_AGD_TOR_U30\": 0,\n",
    "        \"B-SOC_AGD_TOR_B35\": 0,\n",
    "        \"I-SOC_AGD_TOR_B35\": 0,\n",
    "        \"B-SOC_AGD_TOR_A50\": 0,\n",
    "        \"I-SOC_AGD_TOR_A50\": 0,\n",
    "        \"B-SOC_DEV_ATH_M\": 0,\n",
    "        \"I-SOC_DEV_ATH_M\": 0,\n",
    "        \"B-SOC_DEV_ATH_F\": 0,\n",
    "        \"I-SOC_DEV_ATH_F\": 0,\n",
    "        \"B-SOC_OHS_FAT\": 0,\n",
    "        \"I-SOC_OHS_FAT\": 0,\n",
    "        \"B-SOC_OHS_HCI\": 0,\n",
    "        \"I-SOC_OHS_HCI\": 0,\n",
    "        \"B-SOC_OHS_REC\": 0,\n",
    "        \"I-SOC_OHS_REC\": 0,\n",
    "        \"B-SOC_OHS_RWI\": 0,\n",
    "        \"I-SOC_OHS_RWI\": 0,\n",
    "        \"B-GOV_BOC_BIN\": 0,\n",
    "        \"I-GOV_BOC_BIN\": 0,\n",
    "        \"B-GOV_BOC_WOB\": 0,\n",
    "        \"I-GOV_BOC_WOB\": 0,\n",
    "        \"B-GOV_MAD_WMT\": 0,\n",
    "        \"I-GOV_MAD_WMT\": 0,\n",
    "        \"B-GOV_ETB_ACD\": 0,\n",
    "        \"I-GOV_ETB_ACD\": 0,\n",
    "        \"B-GOV_ETB_ACT_N\": 0,\n",
    "        \"I-GOV_ETB_ACT_N\": 0,\n",
    "        \"B-GOV_ETB_ACT_P\": 0,\n",
    "        \"I-GOV_ETB_ACT_P\": 0,\n",
    "        \"B-GOV_CER_LRC\": 0,\n",
    "        \"I-GOV_CER_LRC\": 0,\n",
    "        \"B-GOV_ALF_AFD\": 0,\n",
    "        \"I-GOV_ALF_AFD\": 0,\n",
    "        \"B-GOV_ASS_ASR\": 0,\n",
    "        \"I-GOV_ASS_ASR\": 0,\n",
    "        \"B-VALUE\": 0,\n",
    "        \"I-VALUE\": 0,\n",
    "        \"B-UNIT\": 0,\n",
    "        \"I-UNIT\": 0,\n",
    "        \"O\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取BIO标注数据\n",
    "texts = []\n",
    "labels = []\n",
    "err = []\n",
    "\n",
    "for entry in data:\n",
    "    text = entry['text']\n",
    "    entity_labels = [\"O\"] * len(text)  # 初始化为'O'\n",
    "\n",
    "    for entity in entry['entity']:\n",
    "        start, end, label = entity['start'], entity['end'], entity['labels'][0]\n",
    "        if label not in label_dict:\n",
    "            continue\n",
    "        # 检查字典情况\n",
    "        # if end > len(entity_labels):\n",
    "        #     err.append(data.index(entry))\n",
    "        #     continue\n",
    "        for i in range(start, end):\n",
    "            entity_labels[i] = label\n",
    "\n",
    "    texts.append(list(text))\n",
    "    labels.append(entity_labels)\n",
    "\n",
    "# 将数据转换为 DataFrame 格式\n",
    "df = pd.DataFrame({\"tokens\": texts, \"ner_tags\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 检查函数：检测句子中是否包含至少一个错误标签\n",
    "# def contains_incorrect_label(label_sequence):\n",
    "#     # 如果标签序列全是 'O' 标签，则返回 False（保留该句子）\n",
    "#     if all(label == \"O\" for label in label_sequence):\n",
    "#         return False\n",
    "#     # 如果存在标签不在合法标签集中，则返回 True（表示该句子含有错误标签）\n",
    "#     return any(label not in label_dict for label in label_sequence)\n",
    "\n",
    "# # 找出标错的句子\n",
    "# incorrect_labels_df = df[df['ner_tags'].apply(contains_incorrect_label)]\n",
    "# print(incorrect_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "无实体句子数量: 936\n",
      "实体句子数量: 5153\n",
      "合并后的数据集样本数: 6089\n"
     ]
    }
   ],
   "source": [
    "# 非实体句删除\n",
    "\n",
    "no_entity_data = df[df['ner_tags'].apply(lambda x: all(label == \"O\" for label in x))]\n",
    "entity_data = df[~df['ner_tags'].apply(lambda x: all(label == \"O\" for label in x))]\n",
    "\n",
    "# 保留 15% 的无实体句子\n",
    "no_entity_sample = no_entity_data.sample(frac=0.15, random_state=42)\n",
    "\n",
    "# 合并数据\n",
    "balanced_df = pd.concat([entity_data, no_entity_sample])\n",
    "\n",
    "# 打乱数据集顺序\n",
    "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 检查新的数据分布\n",
    "print(\"无实体句子数量:\", len(no_entity_sample))\n",
    "print(\"实体句子数量:\", len(entity_data))\n",
    "print(\"合并后的数据集样本数:\", len(balanced_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义需要合并的标签字典，将稀有标签映射到新的标签名\n",
    "merge_dict = {\n",
    "    \"B-SOC_AGD_TOR_U30\": \"B-SOC_AGD_TOR\",\n",
    "    \"I-SOC_AGD_TOR_U30\": \"I-SOC_AGD_TOR\",\n",
    "    \"B-SOC_AGD_TOR_B35\": \"B-SOC_AGD_TOR\",\n",
    "    \"I-SOC_AGD_TOR_B35\": \"I-SOC_AGD_TOR\",\n",
    "    \"B-SOC_AGD_TOR_A50\": \"B-SOC_AGD_TOR\",\n",
    "    \"I-SOC_AGD_TOR_A50\": \"I-SOC_AGD_TOR\",\n",
    "    \n",
    "    'B-SOC_AGD_NHI_B35': 'B-SOC_AGD_NHI',\n",
    "    'B-SOC_AGD_NHI_A50': 'B-SOC_AGD_NHI',\n",
    "    'B-SOC_AGD_NHI_U30': 'B-SOC_AGD_NHI',\n",
    "    'I-SOC_AGD_NHI_U30': 'I-SOC_AGD_NHI',\n",
    "    'I-SOC_AGD_NHI_B35': 'I-SOC_AGD_NHI',\n",
    "    'I-SOC_AGD_NHI_A50': 'I-SOC_AGD_NHI',\n",
    "    \n",
    "    'B-ENV_GHG_EI1' : 'B-ENV_GHG_EI',\n",
    "    'I-ENV_GHG_EI1' : 'I-ENV_GHG_EI',\n",
    "    'B-ENV_GHG_EI2' : 'B-ENV_GHG_EI',\n",
    "    'I-ENV_GHG_EI2' : 'I-ENV_GHG_EI',\n",
    "    'B-ENV_GHG_EI3' : 'B-ENV_GHG_EI',\n",
    "    'I-ENV_GHG_EI3' : 'I-ENV_GHG_EI',\n",
    "    \n",
    "    'B-SOC_AGD_CEA_U30' : 'B-SOC_AGD_CEA',\n",
    "    'I-SOC_AGD_CEA_U30' : 'I-SOC_AGD_CEA',\n",
    "    'B-SOC_AGD_CEA_B35' : 'B-SOC_AGD_CEA',\n",
    "    'I-SOC_AGD_CEA_B35' : 'I-SOC_AGD_CEA',\n",
    "    'B-SOC_AGD_CEA_A50' : 'B-SOC_AGD_CEA',\n",
    "    'I-SOC_AGD_CEA_A50' : 'I-SOC_AGD_CEA',\n",
    "    \n",
    "    'B-SOC_GED_ETG_F' : 'B-SOC_GED_ETG',\n",
    "    'I-SOC_GED_ETG_F' : 'I-SOC_GED_ETG',\n",
    "    'B-SOC_GED_ETG_M' : 'B-SOC_GED_ETG',\n",
    "    'I-SOC_GED_ETG_M' : 'I-SOC_GED_ETG',\n",
    "    'B-SOC_GED_NHG_M' : 'B-SOC_GED_NHG',\n",
    "    'I-SOC_GED_NHG_M' : 'I-SOC_GED_NHG',\n",
    "    'B-SOC_GED_NHG_F' : 'B-SOC_GED_NHG',\n",
    "    'I-SOC_GED_NHG_F' : 'I-SOC_GED_NHG'\n",
    "    \n",
    "    # 添加更多需要合并的标签映射\n",
    "}\n",
    "\n",
    "# 定义一个函数，用于将标签序列中的稀有标签合并\n",
    "def merge_labels(label_sequence, merge_dict):\n",
    "    return [merge_dict.get(label, label) for label in label_sequence]\n",
    "\n",
    "# 应用标签合并函数到 DataFrame 的 'labels' 列\n",
    "balanced_df['ner_tags'] = balanced_df['ner_tags'].apply(lambda x: merge_labels(x, merge_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后的标签分布:\n",
      "O                  1153555\n",
      "I-GOV_ALF_AFD        29144\n",
      "B-GOV_ALF_AFD        26855\n",
      "B-VALUE              15533\n",
      "B-GOV_ETB_ACD        10952\n",
      "I-GOV_ETB_ACD         8668\n",
      "B-GOV_BOC_BIN         8135\n",
      "I-GOV_BOC_BIN         8107\n",
      "I-SOC_DEV_ATH_M       6157\n",
      "B-UNIT                5897\n",
      "B-ENV_ENC_TEC         5709\n",
      "B-ENV_GHG_AET         5702\n",
      "B-SOC_DEV_ATH_M       5700\n",
      "I-SOC_OHS_RWI         5246\n",
      "B-SOC_OHS_RWI         5096\n",
      "I-ENV_ENC_TEC         4947\n",
      "I-ENV_GHG_AET         4775\n",
      "I-UNIT                4584\n",
      "I-ENV_WAG_TWG         3689\n",
      "B-SOC_GED_CEG_F       3288\n",
      "B-ENV_WAG_TWG         3052\n",
      "I-GOV_CER_LRC         2911\n",
      "I-SOC_GED_CEG_F       2553\n",
      "I-VALUE               2248\n",
      "B-GOV_CER_LRC         2176\n",
      "I-ENV_WAC_TWC         1971\n",
      "B-ENV_WAC_TWC         1678\n",
      "I-GOV_ASS_ASR         1416\n",
      "B-GOV_ASS_ASR         1349\n",
      "I-ENV_ENC_ECI         1317\n",
      "B-ENV_ENC_ECI         1258\n",
      "I-ENV_GHG_EIT         1161\n",
      "I-ENV_GHG_AE3         1158\n",
      "B-SOC_OHS_REC         1127\n",
      "I-SOC_OHS_REC         1119\n",
      "I-ENV_GHG_AE2         1108\n",
      "I-ENV_GHG_AE1         1080\n",
      "B-ENV_GHG_AE1          996\n",
      "B-GOV_ETB_ACT_N        925\n",
      "I-GOV_ETB_ACT_N        851\n",
      "I-SOC_GED_CEG_M        849\n",
      "B-ENV_GHG_EIT          814\n",
      "B-SOC_GED_CEG_M        762\n",
      "I-GOV_MAD_WMT          730\n",
      "B-GOV_MAD_WMT          686\n",
      "B-ENV_GHG_AE3          658\n",
      "I-GOV_ETB_ACT_P        647\n",
      "B-ENV_GHG_AE2          629\n",
      "B-SOC_DEV_ATH_F        546\n",
      "I-ENV_WAC_WCI          543\n",
      "B-GOV_ETB_ACT_P        526\n",
      "I-SOC_DEV_ATH_F        495\n",
      "B-SOC_OHS_HCI          400\n",
      "B-SOC_AGD_CEA          349\n",
      "B-SOC_OHS_FAT          340\n",
      "B-ENV_WAC_WCI          306\n",
      "I-SOC_AGD_CEA          304\n",
      "I-ENV_GHG_EI           301\n",
      "B-SOC_GED_NHG          293\n",
      "I-SOC_OHS_HCI          259\n",
      "I-SOC_GED_NHG          249\n",
      "I-SOC_OHS_FAT          231\n",
      "B-ENV_GHG_EI           220\n",
      "B-SOC_GED_ETG          181\n",
      "I-GOV_BOC_WOB          128\n",
      "B-GOV_BOC_WOB           99\n",
      "I-SOC_GED_ETG           95\n",
      "B-SOC_AGD_NHI           45\n",
      "I-SOC_AGD_NHI           35\n",
      "B-SOC_AGD_TOR           23\n",
      "I-SOC_AGD_TOR           16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 设置 pandas 的显示选项，防止省略\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# 检查标签合并后的分布\n",
    "all_labels_flat = [item for sublist in balanced_df['ner_tags'] for item in sublist]\n",
    "label_counts_after_merge = pd.Series(all_labels_flat).value_counts()\n",
    "\n",
    "print(\"合并后的标签分布:\")\n",
    "print(label_counts_after_merge)\n",
    "\n",
    "# 恢复默认设置（可选）\n",
    "pd.reset_option('display.max_rows')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 查找标签所在句子\n",
    "# # 目标标签\n",
    "# target_label = \"B-SOC_AGD_TOR\"\n",
    "\n",
    "# # 筛选出包含目标标签的句子\n",
    "# sentences_with_label = balanced_df_resampled[balanced_df_resampled['ner_tags'].apply(lambda x: target_label in x)]\n",
    "\n",
    "# # 查看筛选结果\n",
    "# print(\"包含标签\", target_label, \"的句子数量:\", len(sentences_with_label))\n",
    "# print(sentences_with_label[['tokens', 'ner_tags']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# # 假设 labels 列表中存储了每个文本的标签序列\n",
    "# # 将所有标签展开为一个列表，并使用 Counter 统计每种标签的数量\n",
    "# all_labels = [label for sequence in labels for label in sequence]\n",
    "# label_counts = Counter(all_labels)\n",
    "\n",
    "# # 打印每种实体的数量\n",
    "# for label, count in label_counts.items():\n",
    "#     print(f\"实体标签 '{label}' 的数量为: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过采样后的数据集大小: 7089\n"
     ]
    }
   ],
   "source": [
    "# 定义低频标签的阈值\n",
    "low_count_threshold = 500\n",
    "\n",
    "# 获取所有标签的数量分布\n",
    "all_labels_flat = [item for sublist in balanced_df['ner_tags'] for item in sublist]\n",
    "label_counts = pd.Series(all_labels_flat).value_counts()  # 假设这是一个标签-数量的字典或 Series\n",
    "\n",
    "# 找出所有低频标签\n",
    "low_frequency_labels = [label for label, count in label_counts.items() if count < low_count_threshold]\n",
    "\n",
    "# 初始化一个新的 DataFrame 来存储过采样的句子\n",
    "balanced_df_resampled = balanced_df.copy()\n",
    "\n",
    "# 遍历每一个低频标签，筛选并过采样包含该标签的句子\n",
    "for label in low_frequency_labels:\n",
    "    # 筛选出包含当前标签的句子\n",
    "    sentences_with_label = balanced_df[balanced_df['ner_tags'].apply(lambda x: label in x)]\n",
    "    \n",
    "    # 确认是否需要过采样\n",
    "    if len(sentences_with_label) < low_count_threshold:\n",
    "        # 过采样该标签的句子\n",
    "        sentences_with_label_upsampled = resample(sentences_with_label, \n",
    "                                                  replace=True, \n",
    "                                                  n_samples=50, \n",
    "                                                  random_state=42)\n",
    "        \n",
    "        # 将过采样后的数据合并到主数据集中\n",
    "        balanced_df_resampled = pd.concat([balanced_df_resampled, sentences_with_label_upsampled])\n",
    "\n",
    "# 打乱数据集\n",
    "balanced_df_resampled = balanced_df_resampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# 查看结果\n",
    "print(\"过采样后的数据集大小:\", len(balanced_df_resampled))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后的标签分布:\n",
      "O                  1446378\n",
      "I-GOV_ALF_AFD        36557\n",
      "B-GOV_ALF_AFD        30632\n",
      "B-VALUE              27667\n",
      "B-GOV_ETB_ACD        11621\n",
      "I-UNIT               10829\n",
      "B-UNIT               10115\n",
      "I-GOV_ETB_ACD         9650\n",
      "B-GOV_BOC_BIN         8799\n",
      "I-GOV_BOC_BIN         8334\n",
      "I-SOC_DEV_ATH_M       6850\n",
      "I-ENV_WAG_TWG         6585\n",
      "I-SOC_OHS_RWI         6541\n",
      "B-SOC_DEV_ATH_M       6247\n",
      "B-ENV_ENC_TEC         6170\n",
      "B-SOC_OHS_RWI         6102\n",
      "B-ENV_GHG_AET         5880\n",
      "I-ENV_ENC_TEC         5716\n",
      "I-ENV_GHG_AET         4976\n",
      "B-SOC_GED_CEG_F       4473\n",
      "B-ENV_WAG_TWG         4225\n",
      "I-SOC_GED_CEG_F       3424\n",
      "B-SOC_DEV_ATH_F       3308\n",
      "B-SOC_GED_ETG         3162\n",
      "I-SOC_GED_ETG         3099\n",
      "I-GOV_CER_LRC         2991\n",
      "I-SOC_DEV_ATH_F       2710\n",
      "I-ENV_WAC_TWC         2705\n",
      "B-SOC_OHS_REC         2601\n",
      "B-SOC_GED_NHG         2590\n",
      "I-SOC_OHS_REC         2585\n",
      "B-SOC_OHS_HCI         2563\n",
      "I-VALUE               2505\n",
      "B-SOC_OHS_FAT         2495\n",
      "I-SOC_GED_NHG         2258\n",
      "B-GOV_CER_LRC         2255\n",
      "I-ENV_GHG_EIT         2182\n",
      "I-ENV_GHG_EI          2181\n",
      "I-ENV_ENC_ECI         2136\n",
      "B-ENV_WAC_TWC         2099\n",
      "I-SOC_OHS_FAT         1908\n",
      "I-ENV_WAC_WCI         1905\n",
      "I-SOC_OHS_HCI         1827\n",
      "B-ENV_GHG_EI          1746\n",
      "I-ENV_GHG_AE2         1714\n",
      "B-ENV_ENC_ECI         1560\n",
      "B-ENV_GHG_EIT         1475\n",
      "I-ENV_GHG_AE1         1474\n",
      "I-SOC_AGD_CEA         1444\n",
      "B-SOC_AGD_CEA         1438\n",
      "B-SOC_AGD_TOR         1436\n",
      "I-GOV_ASS_ASR         1416\n",
      "B-GOV_ASS_ASR         1349\n",
      "B-ENV_GHG_AE1         1320\n",
      "B-SOC_AGD_NHI         1290\n",
      "B-SOC_GED_CEG_M       1264\n",
      "I-SOC_GED_CEG_M       1250\n",
      "I-ENV_GHG_AE3         1216\n",
      "I-GOV_BOC_WOB         1207\n",
      "I-SOC_AGD_NHI         1181\n",
      "I-SOC_AGD_TOR         1168\n",
      "B-ENV_WAC_WCI         1096\n",
      "B-GOV_ETB_ACT_N        975\n",
      "I-GOV_ETB_ACT_N        851\n",
      "B-ENV_GHG_AE2          837\n",
      "I-GOV_MAD_WMT          754\n",
      "B-GOV_BOC_WOB          722\n",
      "B-GOV_MAD_WMT          721\n",
      "B-ENV_GHG_AE3          700\n",
      "I-GOV_ETB_ACT_P        647\n",
      "B-GOV_ETB_ACT_P        526\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 设置 pandas 的显示选项，防止省略\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# 检查标签合并后的分布\n",
    "all_labels_flat = [item for sublist in balanced_df_resampled['ner_tags'] for item in sublist]\n",
    "label_counts_after_merge = pd.Series(all_labels_flat).value_counts()\n",
    "\n",
    "print(\"合并后的标签分布:\")\n",
    "print(label_counts_after_merge)\n",
    "\n",
    "# 恢复默认设置（可选）\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转换为 Hugging Face 的 Dataset 格式\n",
    "dataset = Dataset.from_pandas(balanced_df_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有独特标签: ['B-ENV_ENC_ECI', 'B-ENV_ENC_TEC', 'B-ENV_GHG_AE1', 'B-ENV_GHG_AE2', 'B-ENV_GHG_AE3', 'B-ENV_GHG_AET', 'B-ENV_GHG_EI', 'B-ENV_GHG_EIT', 'B-ENV_WAC_TWC', 'B-ENV_WAC_WCI', 'B-ENV_WAG_TWG', 'B-GOV_ALF_AFD', 'B-GOV_ASS_ASR', 'B-GOV_BOC_BIN', 'B-GOV_BOC_WOB', 'B-GOV_CER_LRC', 'B-GOV_ETB_ACD', 'B-GOV_ETB_ACT_N', 'B-GOV_ETB_ACT_P', 'B-GOV_MAD_WMT', 'B-SOC_AGD_CEA', 'B-SOC_AGD_NHI', 'B-SOC_AGD_TOR', 'B-SOC_DEV_ATH_F', 'B-SOC_DEV_ATH_M', 'B-SOC_GED_CEG_F', 'B-SOC_GED_CEG_M', 'B-SOC_GED_ETG', 'B-SOC_GED_NHG', 'B-SOC_OHS_FAT', 'B-SOC_OHS_HCI', 'B-SOC_OHS_REC', 'B-SOC_OHS_RWI', 'B-UNIT', 'B-VALUE', 'I-ENV_ENC_ECI', 'I-ENV_ENC_TEC', 'I-ENV_GHG_AE1', 'I-ENV_GHG_AE2', 'I-ENV_GHG_AE3', 'I-ENV_GHG_AET', 'I-ENV_GHG_EI', 'I-ENV_GHG_EIT', 'I-ENV_WAC_TWC', 'I-ENV_WAC_WCI', 'I-ENV_WAG_TWG', 'I-GOV_ALF_AFD', 'I-GOV_ASS_ASR', 'I-GOV_BOC_BIN', 'I-GOV_BOC_WOB', 'I-GOV_CER_LRC', 'I-GOV_ETB_ACD', 'I-GOV_ETB_ACT_N', 'I-GOV_ETB_ACT_P', 'I-GOV_MAD_WMT', 'I-SOC_AGD_CEA', 'I-SOC_AGD_NHI', 'I-SOC_AGD_TOR', 'I-SOC_DEV_ATH_F', 'I-SOC_DEV_ATH_M', 'I-SOC_GED_CEG_F', 'I-SOC_GED_CEG_M', 'I-SOC_GED_ETG', 'I-SOC_GED_NHG', 'I-SOC_OHS_FAT', 'I-SOC_OHS_HCI', 'I-SOC_OHS_REC', 'I-SOC_OHS_RWI', 'I-UNIT', 'I-VALUE', 'O']\n",
      "71\n"
     ]
    }
   ],
   "source": [
    "# 使用集合存储所有独特标签，避免重复\n",
    "unique_labels = set(label_counts_after_merge.index)\n",
    "\n",
    "# 将集合转换为列表并排序\n",
    "unique_labels = sorted(list(unique_labels))\n",
    "\n",
    "# 查看所有标签\n",
    "print(\"所有独特标签:\", unique_labels)\n",
    "print(len(unique_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = dataset.train_test_split(test_size=0.2,seed = 666)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\esg\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"nbroad/ESG-BERT\"\n",
    "model_name = \"dslim/bert-base-NER\"\n",
    "# model_name = \"dslim/bert-large-NER\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], \n",
    "        truncation=True, \n",
    "        is_split_into_words=True, \n",
    "        padding=True\n",
    "    )\n",
    "    labels = []\n",
    "\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # 忽略位置\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])  # 将标签转换为整数 ID\n",
    "            else:\n",
    "                # 对于当前词的子词部分，通常不需要计算损失，除非你想保持每个子词的相同标签\n",
    "                label_ids.append(label2id[label[word_idx]] if label[word_idx].startswith(\"I-\") else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define compute_metrics function for evaluation\n",
    "def compute_metrics(pred):\n",
    "    # Extract predictions and labels\n",
    "    predictions, labels = pred\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    # Remove ignored index (special tokens)\n",
    "    true_labels = [[label for label, pred in zip(label_row, pred_row) if label != -100] \n",
    "                   for label_row, pred_row in zip(labels, predictions)]\n",
    "    true_predictions = [[pred for label, pred in zip(label_row, pred_row) if label != -100]\n",
    "                        for label_row, pred_row in zip(labels, predictions)]\n",
    "    \n",
    "    # Flatten lists\n",
    "    true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    true_predictions = [item for sublist in true_predictions for item in sublist]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, true_predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, true_predictions, average='weighted')\n",
    "    \n",
    "    # Return results in dictionary\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels),  # 标签数量\n",
    "    id2label=id2label,              # 标签ID到名称的映射\n",
    "    label2id=label2id,              # 标签名称到ID的映射\n",
    "    ignore_mismatched_sizes=True    # 忽略大小不匹配\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8664dd40a21419fbc1d5db6c8196f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d332d002287428abb18dda6486f3025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1418 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\esg\\Lib\\site-packages\\transformers\\training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"../logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Trainer to use compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be9fa043e4e4901bd2fa8528547ecc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\esg\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:439: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5867, 'grad_norm': 2.876352071762085, 'learning_rate': 1.9717912552891397e-05, 'epoch': 0.01}\n",
      "{'loss': 1.1976, 'grad_norm': 1.1854902505874634, 'learning_rate': 1.9435825105782797e-05, 'epoch': 0.03}\n",
      "{'loss': 1.51, 'grad_norm': 1.9207444190979004, 'learning_rate': 1.915373765867419e-05, 'epoch': 0.04}\n",
      "{'loss': 1.3812, 'grad_norm': 6.197670936584473, 'learning_rate': 1.8871650211565585e-05, 'epoch': 0.06}\n",
      "{'loss': 1.4283, 'grad_norm': 2.3361713886260986, 'learning_rate': 1.8589562764456984e-05, 'epoch': 0.07}\n",
      "{'loss': 1.3057, 'grad_norm': 2.5133488178253174, 'learning_rate': 1.830747531734838e-05, 'epoch': 0.08}\n",
      "{'loss': 1.3223, 'grad_norm': 1.8773274421691895, 'learning_rate': 1.8025387870239776e-05, 'epoch': 0.1}\n",
      "{'loss': 1.3961, 'grad_norm': 3.1961872577667236, 'learning_rate': 1.7743300423131172e-05, 'epoch': 0.11}\n",
      "{'loss': 1.3602, 'grad_norm': 1.9935333728790283, 'learning_rate': 1.7461212976022568e-05, 'epoch': 0.13}\n",
      "{'loss': 1.3664, 'grad_norm': 4.477314472198486, 'learning_rate': 1.7179125528913964e-05, 'epoch': 0.14}\n",
      "{'loss': 1.2033, 'grad_norm': 5.197116374969482, 'learning_rate': 1.6897038081805363e-05, 'epoch': 0.16}\n",
      "{'loss': 1.3035, 'grad_norm': 1.8297619819641113, 'learning_rate': 1.661495063469676e-05, 'epoch': 0.17}\n",
      "{'loss': 1.2458, 'grad_norm': 1.3917347192764282, 'learning_rate': 1.633286318758815e-05, 'epoch': 0.18}\n",
      "{'loss': 1.2832, 'grad_norm': 4.161698818206787, 'learning_rate': 1.605077574047955e-05, 'epoch': 0.2}\n",
      "{'loss': 1.3598, 'grad_norm': 1.9021025896072388, 'learning_rate': 1.5768688293370946e-05, 'epoch': 0.21}\n",
      "{'loss': 1.3036, 'grad_norm': 2.2078301906585693, 'learning_rate': 1.5486600846262342e-05, 'epoch': 0.23}\n",
      "{'loss': 1.3401, 'grad_norm': 2.083998918533325, 'learning_rate': 1.520451339915374e-05, 'epoch': 0.24}\n",
      "{'loss': 1.1064, 'grad_norm': 1.7220017910003662, 'learning_rate': 1.4922425952045134e-05, 'epoch': 0.25}\n",
      "{'loss': 1.0988, 'grad_norm': 1.397002935409546, 'learning_rate': 1.4640338504936531e-05, 'epoch': 0.27}\n",
      "{'loss': 1.1501, 'grad_norm': 2.3555853366851807, 'learning_rate': 1.4358251057827927e-05, 'epoch': 0.28}\n",
      "{'loss': 1.2137, 'grad_norm': 2.460810899734497, 'learning_rate': 1.4076163610719325e-05, 'epoch': 0.3}\n",
      "{'loss': 1.3382, 'grad_norm': 4.074743747711182, 'learning_rate': 1.379407616361072e-05, 'epoch': 0.31}\n",
      "{'loss': 1.2597, 'grad_norm': 1.6705167293548584, 'learning_rate': 1.3511988716502115e-05, 'epoch': 0.32}\n",
      "{'loss': 1.2382, 'grad_norm': 1.8988069295883179, 'learning_rate': 1.3229901269393512e-05, 'epoch': 0.34}\n",
      "{'loss': 1.147, 'grad_norm': 3.1599204540252686, 'learning_rate': 1.294781382228491e-05, 'epoch': 0.35}\n",
      "{'loss': 1.1847, 'grad_norm': 2.8575961589813232, 'learning_rate': 1.2665726375176306e-05, 'epoch': 0.37}\n",
      "{'loss': 0.9886, 'grad_norm': 3.735233783721924, 'learning_rate': 1.2383638928067703e-05, 'epoch': 0.38}\n",
      "{'loss': 1.3494, 'grad_norm': 4.780102729797363, 'learning_rate': 1.2101551480959098e-05, 'epoch': 0.39}\n",
      "{'loss': 1.3316, 'grad_norm': 4.246946811676025, 'learning_rate': 1.1819464033850493e-05, 'epoch': 0.41}\n",
      "{'loss': 1.307, 'grad_norm': 4.658580303192139, 'learning_rate': 1.1537376586741891e-05, 'epoch': 0.42}\n",
      "{'loss': 1.2504, 'grad_norm': 2.1673243045806885, 'learning_rate': 1.1255289139633287e-05, 'epoch': 0.44}\n",
      "{'loss': 1.264, 'grad_norm': 2.805096387863159, 'learning_rate': 1.0973201692524684e-05, 'epoch': 0.45}\n",
      "{'loss': 1.1337, 'grad_norm': 4.479288578033447, 'learning_rate': 1.0691114245416079e-05, 'epoch': 0.47}\n",
      "{'loss': 1.1392, 'grad_norm': 2.8133883476257324, 'learning_rate': 1.0409026798307476e-05, 'epoch': 0.48}\n",
      "{'loss': 1.1287, 'grad_norm': 2.9099371433258057, 'learning_rate': 1.0126939351198872e-05, 'epoch': 0.49}\n",
      "{'loss': 1.3948, 'grad_norm': 4.521546840667725, 'learning_rate': 9.84485190409027e-06, 'epoch': 0.51}\n",
      "{'loss': 1.0965, 'grad_norm': 2.094993829727173, 'learning_rate': 9.562764456981665e-06, 'epoch': 0.52}\n",
      "{'loss': 1.2324, 'grad_norm': 3.2397818565368652, 'learning_rate': 9.280677009873061e-06, 'epoch': 0.54}\n",
      "{'loss': 1.3194, 'grad_norm': 2.5051381587982178, 'learning_rate': 8.998589562764459e-06, 'epoch': 0.55}\n",
      "{'loss': 1.2352, 'grad_norm': 1.5151528120040894, 'learning_rate': 8.716502115655853e-06, 'epoch': 0.56}\n",
      "{'loss': 1.2509, 'grad_norm': 2.171860933303833, 'learning_rate': 8.43441466854725e-06, 'epoch': 0.58}\n",
      "{'loss': 1.2712, 'grad_norm': 2.6169426441192627, 'learning_rate': 8.152327221438646e-06, 'epoch': 0.59}\n",
      "{'loss': 1.0194, 'grad_norm': 2.5083816051483154, 'learning_rate': 7.870239774330042e-06, 'epoch': 0.61}\n",
      "{'loss': 1.1656, 'grad_norm': 3.5171732902526855, 'learning_rate': 7.58815232722144e-06, 'epoch': 0.62}\n",
      "{'loss': 1.2357, 'grad_norm': 2.5091052055358887, 'learning_rate': 7.306064880112836e-06, 'epoch': 0.63}\n",
      "{'loss': 1.1792, 'grad_norm': 2.6782639026641846, 'learning_rate': 7.023977433004232e-06, 'epoch': 0.65}\n",
      "{'loss': 1.127, 'grad_norm': 3.4693315029144287, 'learning_rate': 6.741889985895627e-06, 'epoch': 0.66}\n",
      "{'loss': 1.1369, 'grad_norm': 3.064706325531006, 'learning_rate': 6.459802538787024e-06, 'epoch': 0.68}\n",
      "{'loss': 1.1644, 'grad_norm': 2.748748302459717, 'learning_rate': 6.177715091678421e-06, 'epoch': 0.69}\n",
      "{'loss': 1.2785, 'grad_norm': 2.9865922927856445, 'learning_rate': 5.895627644569817e-06, 'epoch': 0.71}\n",
      "{'loss': 1.3563, 'grad_norm': 2.8535354137420654, 'learning_rate': 5.613540197461213e-06, 'epoch': 0.72}\n",
      "{'loss': 0.9279, 'grad_norm': 1.2669575214385986, 'learning_rate': 5.331452750352609e-06, 'epoch': 0.73}\n",
      "{'loss': 0.9512, 'grad_norm': 1.8686391115188599, 'learning_rate': 5.049365303244006e-06, 'epoch': 0.75}\n",
      "{'loss': 1.2302, 'grad_norm': 2.397873640060425, 'learning_rate': 4.767277856135402e-06, 'epoch': 0.76}\n",
      "{'loss': 0.9678, 'grad_norm': 2.298985242843628, 'learning_rate': 4.4851904090267985e-06, 'epoch': 0.78}\n",
      "{'loss': 1.5256, 'grad_norm': 2.838287591934204, 'learning_rate': 4.203102961918195e-06, 'epoch': 0.79}\n",
      "{'loss': 1.1352, 'grad_norm': 1.9136531352996826, 'learning_rate': 3.921015514809591e-06, 'epoch': 0.8}\n",
      "{'loss': 0.9832, 'grad_norm': 2.998838424682617, 'learning_rate': 3.6389280677009874e-06, 'epoch': 0.82}\n",
      "{'loss': 0.9706, 'grad_norm': 1.530892252922058, 'learning_rate': 3.3568406205923837e-06, 'epoch': 0.83}\n",
      "{'loss': 1.2892, 'grad_norm': 2.5787205696105957, 'learning_rate': 3.0747531734837804e-06, 'epoch': 0.85}\n",
      "{'loss': 1.144, 'grad_norm': 1.983960509300232, 'learning_rate': 2.7926657263751767e-06, 'epoch': 0.86}\n",
      "{'loss': 1.3048, 'grad_norm': 1.738198161125183, 'learning_rate': 2.510578279266573e-06, 'epoch': 0.87}\n",
      "{'loss': 1.2391, 'grad_norm': 2.1967568397521973, 'learning_rate': 2.2284908321579692e-06, 'epoch': 0.89}\n",
      "{'loss': 1.0178, 'grad_norm': 3.7401397228240967, 'learning_rate': 1.9464033850493655e-06, 'epoch': 0.9}\n",
      "{'loss': 1.0093, 'grad_norm': 2.8298287391662598, 'learning_rate': 1.6643159379407616e-06, 'epoch': 0.92}\n",
      "{'loss': 0.9247, 'grad_norm': 2.0631890296936035, 'learning_rate': 1.382228490832158e-06, 'epoch': 0.93}\n",
      "{'loss': 1.168, 'grad_norm': 2.097446918487549, 'learning_rate': 1.1001410437235544e-06, 'epoch': 0.94}\n",
      "{'loss': 1.1881, 'grad_norm': 2.9058427810668945, 'learning_rate': 8.180535966149506e-07, 'epoch': 0.96}\n",
      "{'loss': 0.9921, 'grad_norm': 1.9858335256576538, 'learning_rate': 5.35966149506347e-07, 'epoch': 0.97}\n",
      "{'loss': 1.2443, 'grad_norm': 3.176978588104248, 'learning_rate': 2.538787023977433e-07, 'epoch': 0.99}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4dd08fac504460932c085c1ceb9643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1157557964324951, 'eval_accuracy': 0.7926746118262844, 'eval_precision': 0.64166726852883, 'eval_recall': 0.7926746118262844, 'eval_f1': 0.7077866833121764, 'eval_runtime': 152.2816, 'eval_samples_per_second': 9.312, 'eval_steps_per_second': 1.169, 'epoch': 1.0}\n",
      "{'train_runtime': 510.8828, 'train_samples_per_second': 11.1, 'train_steps_per_second': 1.388, 'train_loss': 1.2307396378940856, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\esg\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=709, training_loss=1.2307396378940856, metrics={'train_runtime': 510.8828, 'train_samples_per_second': 11.1, 'train_steps_per_second': 1.388, 'total_flos': 1482738299685888.0, 'train_loss': 1.2307396378940856, 'epoch': 1.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d000b702621b46e4a558d6c743867ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.1157557964324951, 'eval_accuracy': 0.7926746118262844, 'eval_precision': 0.64166726852883, 'eval_recall': 0.7926746118262844, 'eval_f1': 0.7077866833121764, 'eval_runtime': 26.9121, 'eval_samples_per_second': 52.69, 'eval_steps_per_second': 6.614, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\esg\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('../finetuned_model\\\\tokenizer_config.json',\n",
       " '../finetuned_model\\\\special_tokens_map.json',\n",
       " '../finetuned_model\\\\vocab.txt',\n",
       " '../finetuned_model\\\\added_tokens.json',\n",
       " '../finetuned_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('../finetuned_model')\n",
    "tokenizer.save_pretrained('../finetuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载微调后的模型和分词器\n",
    "model_path = \"../finetuned_model\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertForTokenClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"../logs\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# 可以进行增量训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
